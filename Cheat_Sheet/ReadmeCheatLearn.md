<a name='top'></a>
[Principal](../README.md)<br/>

[Documentaci√≥n])<https://www.interactivechaos.com/python/function/traintestsplit>


[Api_Post](READMEPOST.md) | [Api_Get](READMEGET.md)  | [Tuplas](READMETupleSet.md) | [Listas](READMELIST.md) | [Diccionarios](READMEDIC.md) | [Selenium](../Selenium/README.md)


*[scikit-learn -> regresi√≥n lineal multiple](#multiple)*<br/>
*[scikit-learn -> regresi√≥n polin√≥mica](#polinomica)*<br/>
*[scikit-learn -> regresi√≥n no lineal](#noLineal)*<br/>
*[scikit-learn -> regresi√≥n log√≠stica](#RegrasionLogistica)*<br/>

<a name='lineal'></a>
# Librer√≠a scikit-learn para implementar regresi√≥n lineal simple:
#### Descargaremos un set de datos relacionado al consumo de combustible y a la emisi√≥n del di√≥xido de Carbono en autos.  Luego, separaremos nuestros datos en un set de entrenamiento y en otro set de prueba, crearemos un modelo utilizando un set de entrenamiento, se evaluar√° utilizando el set de prueba para finalmente usar el modelo para predecir valores desconocidos
### Importando paquetes Necesarios:

    import matplotlib.pyplot as plt
    import pandas as pd
    import pylab as pl
    import numpy as np
    %matplotlib inline

### Descargando los Datos:
#### Para descargar los datos, usaremos !wget desde IBM Object Storage.
    
    !wget -O FuelConsumption.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv

#### ¬øSab√≠as? Cuando se trata de Machine Learning, seguro trabajar√°s con grandes datasets (juego de datos). Entonces, ¬ød√≥nde podr√°s guardar esos datos? IBM ofrece una oportunidad √∫nica para las empresas, con 10 Tb de IBM Cloud Object Storage: Sign up now for free

##### Understanding the Data
#### FuelConsumption.csv:
#### Hemos descargado el dataset de consumo de combustible, FuelConsumption.csv, el cual contiene ratings espec√≠ficos al consumo de combustible y emisiones de di√≥xido de carbono para aquellos veh√≠culos ligeros en la venta minorista dentro de Canad√°. Dataset source

    MODELYEAR e.g. 2014
    MAKE e.g. Acura
    MODEL e.g. ILX
    VEHICLE CLASS e.g. SUV
    ENGINE SIZE e.g. 4.7
    CYLINDERS e.g 6
    TRANSMISSION e.g. A6
    FUEL CONSUMPTION in CITY(L/100 km) e.g. 9.9
    FUEL CONSUMPTION in HWY (L/100 km) e.g. 8.9
    FUEL CONSUMPTION COMB (L/100 km) e.g. 9.2
    CO2 EMISSIONS (g/km) e.g. 182 --> low --> 0
   
### Leyendo los datos:
    df = pd.read_csv("FuelConsumption.csv")
    #un vistazo dentro del set de datos
    df.head()
    
### Exploraci√≥n de Datos:
    # Tengamos primero una exploraci√≥n descriptiva de nuestros datos.
    # Sumarizar los datos
    df.describe()
### Seleccionemos algunas caracter√≠sticas para explorar m√°s en detalle:
    cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]
    cdf.head(9)
### podemos dibujar cada una de estas caracter√≠sticas:
    viz = cdf[['CYLINDERS','ENGINESIZE','CO2EMISSIONS','FUELCONSUMPTION_COMB']]
    viz.hist()
    plt.show()
### Ahora, comparemos estas caracter√≠sticas anteriores con la emisi√≥n de carbono, para ver cu√°n lineal es la regresi√≥n:
    plt.scatter(cdf.FUELCONSUMPTION_COMB, cdf.CO2EMISSIONS,  color='blue')
    plt.xlabel("FUELCONSUMPTION_COMB")
    plt.ylabel("Emission")
    plt.show()
    
    plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color='blue')
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    plt.show()
    
## Creando el set de datos de entrenamiento y de el prueba:
#### Train/Test Split divide el dataseet en uno de entrenamiento y otro de pruebas, siendo excluyentes. Despu√©s de ello, entrenas con el set de entrenamiento y pruebas con el de prueba. 
#### Esto brinda una evaluaci√≥n m√°s exacta porque el set de entrenamiento no es parte de un set de datos que se usaron para entrenar datos. Refleja un escenario m√°s real basado en problemas m√°s actuales.
#### Esto significa que sabemos la salida de cada punto de datos del set, siendo un escenario ideal ! Y como estos datos no se usaron para entrenar el modelo, el modelo no sabe la salida de estos puntos de datos. Asi que, b√°sicamente, es una real prueba fuera de muestra.

    msk = np.random.rand(len(df)) < 0.8
    train = cdf[msk]
    test = cdf[~msk]
    
## Modelo de Regresi√≥n Simple:
#### La Regresi√≥n Lineal cuadra con un modelo lineal de coeficientes B = (B1, ..., Bn) para minimizar la 'suma residual de cuadrados' entre la x independiente del dataset y la dependiente y por la aproximaci√≥n lineal.

### Entrenar distribuci√≥n de los datos:
    plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    plt.show()

## Modeling
### Usando el paquete sklearn para modelar datos:
    from sklearn import linear_model
    regr = linear_model.LinearRegression()
    train_x = np.asanyarray(train[['ENGINESIZE']])
    train_y = np.asanyarray(train[['CO2EMISSIONS']])
    regr.fit (train_x, train_y)
    # The coefficients
    print ('Coefficients: ', regr.coef_)
    print ('Intercept: ',regr.intercept_)

#### son los par√°metros de la recta de ajuste. 
#### Dado que es una regresi√≥n lineal simple, con 2 par√°metros solamente, y sabiendo que los par√°metros son la intersecci√≥n y pendiente de la linea, sklearn puede estimarlas directamente a partir de los datos. 
#### Tener en cuenta que todos los datos deben estar disponibles para poder calcular los par√°metros.

## Trazar las salidas
### podemos marcar la recta de ajuste sobre los datos:
    plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
    plt.plot(train_x, regr.coef_[0][0]*train_x + regr.intercept_[0], '-r')
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    
## Evaluaci√≥n
#### comparamos los valores actuales y predichos para calcular la exactitud del modelo de regresi√≥n. Las m√©tricas de la evaluaci√≥n proveen un role principal en el #### desarrollo de un modelo, ya que provee conocimiento profundo en areas que necesitan mejoras.
    from sklearn.metrics import r2_score

    test_x = np.asanyarray(test[['ENGINESIZE']])
    test_y = np.asanyarray(test[['CO2EMISSIONS']])
    test_y_ = regr.predict(test_x)

    print("Error medio absoluto: %.2f" % np.mean(np.absolute(test_y_ - test_y)))
    print("Suma residual de los cuadrados (MSE): %.2f" % np.mean((test_y_ - test_y) ** 2))
    print("R2-score: %.2f" % r2_score(test_y_ , test_y) )

# Librer√≠a scikit-learn para implementar regresi√≥n lineal multiple:
### Importar paquetes necesarios:
    import matplotlib.pyplot as plt
    import pandas as pd
    import pylab as pl
    import numpy as np
    %matplotlib inline
### Downloading Data:
    !wget -O FuelConsumption.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv
### Reading the data in:
    df = pd.read_csv("FuelConsumption.csv")
    # Dale un vistazo al conjunto de datos
    df.head()
### Seleccionemos algunas caracter√≠sticas:
    cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_CITY','FUELCONSUMPTION_HWY','FUELCONSUMPTION_COMB','CO2EMISSIONS']]
    cdf.head(9)
### Tracemos los valores de las emisiones con respecto al tama√±o del motor:
    plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color='blue')
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    plt.show()

## Creating train and test dataset
#### La divisi√≥n tren/prueba implica dividir el conjunto de datos en conjuntos de formaci√≥n y de pruebas respectivamente, que son mutuamente excluyentes. Despu√©s de lo cual, usted entrena con el equipo de entrenamiento y prueba con el equipo de prueba. Esto proporcionar√° una evaluaci√≥n m√°s precisa de la precisi√≥n fuera de la muestra, ya que el conjunto de datos de la prueba no forma parte del conjunto de datos que se ha utilizado para entrenar los datos. Es m√°s realista para los problemas del mundo real.
    msk = np.random.rand(len(df)) < 0.8
    train = cdf[msk]
    test = cdf[~msk]
#### Train data distribution
    plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    plt.show()
    
    
<a name='multiple'></a>
*[scikit-learn -> regresi√≥n lineal](#lineal)*<br/>
*[scikit-learn -> regresi√≥n no lineal](#noLineal)*<br/>
*[scikit-learn -> regresi√≥n polin√≥mica](#polinomica)*

## Multiple Regression Model: 
#### Cuando hay m√°s de una variable independiente presente, el proceso se denomina regresi√≥n lineal m√∫ltiple. Lo bueno aqu√≠ es que la regresi√≥n lineal m√∫ltiple es la extensi√≥n del modelo de regresi√≥n lineal simple.
    from sklearn import linear_model
    regr = linear_model.LinearRegression()
    x = np.asanyarray(train[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])
    y = np.asanyarray(train[['CO2EMISSIONS']])
    regr.fit (x, y)
    # The coefficients
    print ('Coefficients: ', regr.coef_)

## Ordinary Least Squares (OLS)
#### OLS es un m√©todo para estimar los par√°metros desconocidos en un modelo de regresi√≥n lineal. OLS elige los par√°metros de una funci√≥n lineal de un conjunto de variables explicativas minimizando la suma de los cuadrados de las diferencias entre la variable objetivo dependiente y las previstas por la funci√≥n lineal. En otras palabras, intenta minimizar la suma de errores cuadrados (SSE) o el error cuadrado medio (MSE) entre la variable objetivo (y) y nuestro resultado previsto ( ‚Ñéùëéùë°‚Ñéùëéùë°ùë¶ ) en todas las muestras del conjunto de datos.

#### OLS puede encontrar los mejores par√°metros usando los siguientes m√©todos: - Resoluci√≥n anal√≠tica de los par√°metros del modelo mediante ecuaciones de forma cerrada - Utilizando un algoritmo de optimizaci√≥n (Descenso de Gradiente, Descenso de Gradiente Estoc√°stico, M√©todo de Newton, etc.)

#### Prediction:
    y_hat= regr.predict(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])
    x = np.asanyarray(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])
    y = np.asanyarray(test[['CO2EMISSIONS']])
    print("Residual sum of squares: %.2f"
      % np.mean((y_hat - y) ** 2))

    # Explained variance score: 1 is perfect prediction
    print('Variance score: %.2f' % regr.score(x, y))


<a name='polinomica'></a>
*[scikit-learn -> regresi√≥n lineal](#lineal)*<br/>
*[scikit-learn -> regresi√≥n lineal multiple](#multiple)*<br/>
*[scikit-learn -> regresi√≥n no lineal](#noLineal)*<br/>

## Regresi√≥n polin√≥mica:
#### Implementar una Regresi√≥n Polin√≥mica. Descargaremos un set de datos relacionado al consumo de combustible y a la emisi√≥n del di√≥xido de Carbono en autos. Luego, separaremos nuestros datos en un set de entrenamiento y en otro set de prueba, crearemos un modelo utilizando un set de entrenamiento, se evaluar√° utilizando el set de prueba para finalmente usar el modelo para predecir valores desconocidos

### Importando los paquetes necesarios:
    import matplotlib.pyplot as plt
    import pandas as pd
    import pylab as pl
    import numpy as np
    %matplotlib inline

### Descarga de Datos:
    !wget -O FuelConsumption.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv

### Entender los Datos:
##### FuelConsumption.csv:
#### Hemos descargado el dataset de consumo de combustible, FuelConsumption.csv, el cual contiene ratings espec√≠ficos al consumo de combustible y emisiones de di√≥xido de carbono para aquellos veh√≠culos ligeros en la venta minorista dentro de Canad√°. Fuente de Datos

    MODELYEAR (A√±o del modelo) e.g. 2014
    MAKE (fabricante) e.g. Acura    
    MODEL (modelo) e.g. ILX
    VEHICLE CLASS (tipo de vehiculo) e.g. SUV
    ENGINE SIZE (tama√±o del motor) e.g. 4.7
    CYLINDERS (cilindrada) e.g 6
    TRANSMISSION (transmisi√≥n) e.g. A6
    FUEL CONSUMPTION in CITY(L/100 km) (consumo en ciudad) e.g. 9.9
    FUEL CONSUMPTION in HWY (L/100 km) (consumo en carretera) e.g. 8.9
    FUEL CONSUMPTION COMB (L/100 km) (consumo mixto) e.g. 9.2
    CO2 EMISSIONS (g/km) (emisiones de dioxido de carbono) e.g. 182 --> low --> 0

### Leyendo los datos:
    df = pd.read_csv("FuelConsumption.csv")
    # observar dentro del conjunto de datos
    df.head()

### Seleccionemos algunas caracater√≠sticas para usar en la regresi√≥n:
    cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]
    cdf.head(9)
    
### Grafiquemos los valores de emisi√≥n respecto al tama√±o del motor:
    plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color='blue')
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    plt.show()
    
### Crear conjunto de datos de entrenamiento y pruebas:
#### Hay que dividir el conjunto en dos, el de entrenamiento y el de pruebas, los cuales son mutuamente excluyentes. Despues de hacerlo, deber√° entrenar con el conjunto de entrenamiento y hacer pruebas con el conjunto de pruebas.
    msk = np.random.rand(len(df)) < 0.8
    train = cdf[msk]
    test = cdf[~msk]

## Regresi√≥n Polin√≥mica:
#### En ocasiones la tendencia de los datos no es lineal si no que tiene una apariencia curva. Para estos caso podemos usar los m√©todos de Regresi√≥n Polin√≥mica. De hecho, existen diversos tipos de regresi√≥n que pueden ser usados para ajustarse de acuerdo a la apariencia de los datos, como puede ser la regresi√≥n cuadratica, c√∫bica, etc. Puede haber tantos tipos de regresiones como grados en un polinomio.
#### La funci√≥n PloynomialFeatures() de la librer√≠a Scikit-learn maneja un nuevo conjunto de caracter√≠sticas del conjunto original.

    from sklearn.preprocessing import PolynomialFeatures
    from sklearn import linear_model
    train_x = np.asanyarray(train[['ENGINESIZE']])
    train_y = np.asanyarray(train[['CO2EMISSIONS']])

    test_x = np.asanyarray(test[['ENGINESIZE']])
    test_y = np.asanyarray(test[['CO2EMISSIONS']])

    poly = PolynomialFeatures(degree=2)
    #    fit_transform toma los valores de x e imprime una lista de los datos que van desde la magnitud 0 a la 2 (ya que hemos seleccionado que nuestro polin√≥mio   sea de segundo grado).
    train_x_poly = poly.fit_transform(train_x)
    train_x_poly

#### Ahora podemos manejar el problema como si se tratara de una 'regresi√≥n lineal'. Por lo tanto, esta regresi√≥n polinomica se considera como un caso especial de regresi√≥n lineal m√∫ltiple. Puede utilizar la misma mec√°nica para resolver dicho problema.
### Usemos la funci√≥n LinearRegression() para resolver:
    clf = linear_model.LinearRegression()
    train_y_ = clf.fit(train_x_poly, train_y)
    # los coeficientes 
    print ('Coefficients: ', clf.coef_)
    print ('Intercept: ',clf.intercept_)

### Grafiquemoslo:
    plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
    XX = np.arange(0.0, 10.0, 0.1)
    yy = clf.intercept_[0]+ clf.coef_[0][1]*XX+ clf.coef_[0][2]*np.power(XX, 2)
    plt.plot(XX, yy, '-r' )
    plt.xlabel("Engine size")
    plt.ylabel("Emission")

### Evaluaci√≥n:
    from sklearn.metrics import r2_score

    test_x_poly = poly.fit_transform(test_x)
    test_y_ = clf.predict(test_x_poly)

    print("Mean absolute error: %.2f" % np.mean(np.absolute(test_y_ - test_y)))
    print("Residual sum of squares (MSE): %.2f" % np.mean((test_y_ - test_y) ** 2))
    print("R2-score: %.2f" % r2_score(test_y_ , test_y) )
    
### uso de la regresi√≥n polinomica de tercer grado(c√∫bica) para mayor precisi√≥n:
    poly3 = PolynomialFeatures(degree=3)
    train_x_poly3 = poly3.fit_transform(train_x)
    clf3 = linear_model.LinearRegression()
    train_y3_ = clf3.fit(train_x_poly3, train_y)
    # The coefficients
    print ('Coefficients: ', clf3.coef_)
    print ('Intercept: ',clf3.intercept_)
    plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
    XX = np.arange(0.0, 10.0, 0.1)
    yy = clf3.intercept_[0]+ clf3.coef_[0][1]*XX + clf3.coef_[0][2]*np.power(XX, 2) + clf3.coef_[0][3]*np.power(XX, 3)
    plt.plot(XX, yy, '-r' )
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    test_x_poly3 = poly3.fit_transform(test_x)
    test_y3_ = clf3.predict(test_x_poly3)
    print("Mean absolute error: %.2f" % np.mean(np.absolute(test_y3_ - test_y)))
    print("Residual sum of squares (MSE): %.2f" % np.mean((test_y3_ - test_y) ** 2))
    print("R2-score: %.2f" % r2_score(test_y3_ , test_y) )

<a name='noLineal'></a>
*[scikit-learn -> regresi√≥n lineal](#lineal)*<br/>
*[scikit-learn -> regresi√≥n lineal multiple](#multiple)*<br/>
*[scikit-learn -> regresi√≥n polin√≥mica](#polinomica)*
## Regresion no lienal:
### Importando las librer√≠as requeridas:
    import numpy as np
    import matplotlib.pyplot as plt
    %matplotlib inline

#### Las regresiones no-lineales son una relaci√≥n entre variables independientes  ùë•  y una variable dependiente  ùë¶  que resulta en una funci√≥n no lineal. B√°sicamente, cada relaci√≥n que no es lineal puede transformarse en una no lineal, y generalmente se representa con el polinomio de grados  ùëò  (potencia m√°xima de  ùë• ).
    ùë¶=ùëéùë•3+ùëèùë•2+ùëêùë•+ùëë 
 
####Las funciones no lineales pueden tener elementos como exponentes, logaritmos, fracciones y otros. Por ejemplo:
    ùë¶=log(ùë•)
 
#### O m√°s complicados, como :
    ùë¶=log(ùëéùë•3+ùëèùë•2+ùëêùë•+ùëë)

### gr√°fico de la funci√≥n c√∫bica:
    x = np.arange(-5.0, 5.0, 0.1)

    ##Puede ajustar la pendiente y la intersecci√≥n para verificar los cambios del gr√°fico
    y = 1*(x**3) + 1*(x**2) + 1*x + 3
    y_noise = 20 * np.random.normal(size=x.size)
    ydata = y + y_noise
    plt.plot(x, ydata,  'bo')
    plt.plot(x,y, 'r') 
    plt.ylabel('Variable dependiente')
    plt.xlabel('Variable indepdendiente')
    plt.show()
    
#### Como se puede ver, esta funci√≥n tiene  ùë•3  y  ùë•2  como variables independientes. Tambi√©n, el gr√°fico de esta funci√≥n no es una linea directa, por lo que es una funci√≥n no lineal.

## Algunas otras funciones no lineales son:
### Cuadr√°tica ùëå=ùëã2
    x = np.arange(-5.0, 5.0, 0.1)

    ##Se puede ajustar la pendiente y la intersecci√≥n para verificar los cambios en el gr√°fico
    y = np.power(x,2)
    y_noise = 2 * np.random.normal(size=x.size)
    ydata = y + y_noise
    plt.plot(x, ydata,  'bo')
    plt.plot(x,y, 'r') 
    plt.ylabel('Variable dependiente')
    plt.xlabel('Variable indepdiendente')
    plt.show()

### Exponencial
#### Una funci√≥n exponencial con base c se define por ùëå=ùëé+ùëèùëêùëã
#### donde b ‚â†0, c > 0 , c ‚â†1, y x es cualquier n√∫mero real. La base, c, es constante y el exponente, x, es una variable.
    X = np.arange(-5.0, 5.0, 0.1)

    ##Se puede ajustar la pendiente y la intersecci√≥n para verificar los cambios en el gr√°fico
    Y= np.exp(X)
    plt.plot(X,Y) 
    plt.ylabel('Variable Dependiente')
    plt.xlabel('Variable Independiente')
    plt.show()
    
### Logar√≠tmico
#### La respuesta  ùë¶  es el resultado de aplicar el mapa logar√≠tmico desde el valor de entrada de  ùë•  a la variable de salida  ùë¶ . Es una de las formas m√°s simples de log(): i.e. ùë¶=log(ùë•)
 
#### considerar que en vez de  ùë• , podemos usar  ùëã , el cual puede ser una representaci√≥n polinomial de las  ùë• 's. En su forma general, se escribir√≠a como ùë¶=log(ùëã)
    X = np.arange(-5.0, 5.0, 0.1)

    Y = np.log(X)
    plt.plot(X,Y) 
    plt.ylabel('Variable Dependiente')
    plt.xlabel('Variable Independiente')
    plt.show()
    
### Sigmoidal/Log√≠stica ùëå=ùëé+ùëè1+ùëê(ùëã‚àíùëë)
    X = np.arange(-5.0, 5.0, 0.1)

    Y = 1-4/(1+np.power(3, X-2))

    plt.plot(X,Y) 
    plt.ylabel('Variable Dependiente')
    plt.xlabel('Variable Independiente')
    plt.show()
    
### Ejemplo Regresi√≥n No-Lineal:
#### Por ejemplo, intentaremos fijar un modelo no lineal a los puntos correspondientes al GDP de China entre los a√±os 1960 y 2014. Descargaremos un set de datos con dos columnas, la primera, un a√±o entre 1960 y 2014, la segunda, el ingreso anual de China en d√≥lares estadounidenses para ese a√±o.
    import numpy as np
    import pandas as pd

    #downloading dataset
    !wget -nv -O china_gdp.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/china_gdp.csv
    
    df = pd.read_csv("china_gdp.csv")
    df.head(10)
    
### Marcando el set de datos:
    plt.figure(figsize=(8,5))
    x_data, y_data = (df["Year"].values, df["Value"].values)
    plt.plot(x_data, y_data, 'ro')
    plt.ylabel('GDP')
    plt.xlabel('Year')
    plt.show()
    
### Eligiendo un modelo:
    X = np.arange(-5.0, 5.0, 0.1)
    Y = 1.0 / (1.0 + np.exp(-X))

    plt.plot(X,Y) 
    plt.ylabel('Variable Dependiente')
    plt.xlabel('Variable Independiente')
    plt.show()
    
### Construyendo el Modelo:
    def sigmoid(x, Beta_1, Beta_2):
        y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))
        return y

### sigmoide posible:
    beta_1 = 0.10
    beta_2 = 1990.0

    #funci√≥n log√≠stica
    Y_pred = sigmoid(x_data, beta_1 , beta_2)

    #predicci√≥n de puntos
    plt.plot(x_data, Y_pred*15000000000000.)
    plt.plot(x_data, y_data, 'ro')
    
### busqueda de mejores par√°metros y normalizar x e y:
    # Normalicemos nuestros datos
    xdata =x_data/max(x_data)
    ydata =y_data/max(y_data)

### ¬øC√≥mo podemos encontrar los mejores par√°metros para nuestra linea?
#### podemos utilizar curve_fit la cual utiliza cuadrados m√≠nimos no lineales para cuadrar con la funci√≥n sigmoide
#### popt son nuestros par√°metros optimizados.
    from scipy.optimize import curve_fit
    popt, pcov = curve_fit(sigmoid, xdata, ydata)
    #imprimir los par√°metros finales
    print(" beta_1 = %f, beta_2 = %f" % (popt[0], popt[1]))
    
### Dibujamos nuestro modelo de regresi√≥n:
    x = np.linspace(1960, 2015, 55)
    x = x/max(x)
    plt.figure(figsize=(8,5))
    y = sigmoid(x, *popt)
    plt.plot(xdata, ydata, 'ro', label='data')
    plt.plot(x,y, linewidth=3.0, label='fit')
    plt.legend(loc='best')
    plt.ylabel('GDP')
    plt.xlabel('Year')
    plt.show()

###  calcular la exactitud del modelo:
    msk = np.random.rand(len(df)) < 0.8
    train_x = xdata[msk]
    test_x = xdata[~msk]
    train_y = ydata[msk]
    test_y = ydata[~msk]

    # construye el modelo utilizando el set de entrenamiento
    popt, pcov = curve_fit(sigmoid, train_x, train_y)

    # predecir utilizando el set de prueba
    y_hat = sigmoid(test_x, *popt)

### evaluation:
    print("Promedio de error absoluto: %.2f" % np.mean(np.absolute(y_hat - test_y)))
    print("Suma residual de cuadrados (MSE): %.2f" % np.mean((y_hat - test_y) ** 2))
    from sklearn.metrics import r2_score
    print("R2-score: %.2f" % r2_score(y_hat , test_y) )

<a name='RegrasionLogistica'></a>
## REGRESION LOG√çSTICA:
crear un modelo basado en datos de telecomunicaciones para predecir cu√°ndo los clientes buscar√°n otro competidor de forma tal de poder tomar alguna decisi√≥n para retenerlos.

#### ¬øCu√°l es la diferencia entre Regresi√≥n Log√≠stica y Regresi√≥n Lineal?
Mientras la Regresi√≥n Lineal es para estimar valores continuos (ej. estimar precios de casas), no es la mejor herramienta para predecir la clase de un punto de datos observados. Para estimar la clase de punto de datos, necesitaremos una gu√≠a de lo que ser√≠a la clase m√°s probable para ese punto de datos. Por esto, utilizamos Regresi√≥n Log√≠stica.

### Regresi√≥n Lineal:

Como sabes, la __Regresi√≥n lineal__ encuentra una funci√≥n que relaciona una variable continua dependiente, _y_, con algunos predictores (variables independientes _x1_, _x2_, etc.). Por ejemplo, la regresi√≥n lineal Simple asume una funci√≥n de la forma:

    ùë¶=ùúÉ0+ùúÉ1‚àóùë•1+ùúÉ2‚àóùë•2+...
 

y encuentra los valores de los par√°metros _Œ∏0_, _Œ∏1_, _ùúÉ2_, etc, donde el t√©rmino _ùúÉ0_ es "intersecci√≥n". Generalmente se muestra como:

    ‚ÑéŒ∏(ùë•)=ùúÉùëáùëã
    
La Regresion Log√≠stica es una variaci√≥n de una Regresi√≥n Lineal, √∫til cuando la variable dependiente observada, y, es categ√≥rica. Produce una f√≥rmula que predice la probabilidad de la clase etiqueta como una funci√≥n de las variables independientes.

La regresi√≥n log√≠stica es una curva especial en forma de s a partir de tomar la regresi√≥n lineal y transformar la estimaci√≥n num√©rica en una probabilidad

En resumen, la Regresi√≥n Log√≠stica pasa la entrada a trav√©s de las funciones log√≠stica/sigmoide pero en realidad termina tratando al resultado como una probabilidad:

<Regresion log√≠stica.png>

### Cliente churn con Regresi√≥n Log√≠stica
Una compa√±√≠a de telecomunicaciones est√° preocupada por el n√∫mero de clientes que dejan sus l√≠neas fijas de negocio por las de competidores de cable. Ellos necesitan entender qui√©n se est√° yendo. Imagina que eres un analista en esta compa√±√≠a y que tienes que descubrir qui√©n es el cliente que se va y por qu√©

### importamos librer√≠as:
    import pandas as pd
    import pylab as pl
    import numpy as np
    import scipy.optimize as opt
    from sklearn import preprocessing
    %matplotlib inline 
    import matplotlib.pyplot as plt

### Acerca del set de datos:
Utilizaremos datos de las telecomunicaciones para poder predecir el cliente churn. Estos son datos hist√≥ricos de clientes donde cada fila representa un cliente. Los datos son f√°ciles de comprender, y podr√°s descubrir conclusiones que puedes usar de inmediato. Generalmente, es menos caro mantener clientes que conseguir nuevos, as√≠ que el foco de este an√°lisis es predecir los clientes que se quedar√≠an en la compa√±√≠a.
Los datos incluyen informaci√≥n acerca de:

- Clientes que se fueron el √∫ltimo mes ‚Äì la columna se llama Churn
- Los servicios que cada cliente ha contratado ‚Äì tel√©fono, l√≠neas m√∫ltiples, internet, seguridad online, resguardo online, protecci√≥n de dispositivos, soporte t√©cnico y streaming de TV y pel√≠culas
- Informaci√≥n de la cuenta del cliente - cu√°nto hace que es cliente, contrato, m√©todo de pago, facturaci√≥n digital, cargos mensuales y cargos totales
- Informaci√≥n demogr√°fica de los clientes ‚Äì sexo, rango de edad y si tienen pareja y dependientes

### Cargar los datos Churn de la Telco:
Telco Churn es un archivo de datos ficticio que trata sobre los esfuerzos de una compa√±√≠a de telecomunicaciones para reducir la hu√≠da de sus clientes. Cada caso corresponde a un cliente y se guarda informaci√≥n demogr√°fica e informaci√≥n referente al uso del servicio. Antes de trabajar con los datos, debes utilizar la URL para obtener el archivo ChurnData.csv.

    !wget -O ChurnData.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/ChurnData.csv
    
### Cargar los Datos desde el Archivo CSV:
    churn_df = pd.read_csv("ChurnData.csv")
    churn_df.head()

### Selecci√≥n y pre-procesamiento de datos:
Seleccionemos algunas caracter√≠sticas para el modelado. Tambi√©n cambiemos el tipo de dato del objetivo (target) para que sea un n√∫mero entero (integer), ya que es un requerimiento del algoritmo skitlearn:

    churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]
    churn_df['churn'] = churn_df['churn'].astype('int')
    churn_df.head()
    
### contamos el n√∫mero de filas y columnas total:
    print(churn_df.count)
    
### Definamos X, e y para nuestro set de datos:
    X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])
    X[0:5]
    
    y = np.asarray(churn_df['churn'])
    y [0:5]
### normalicemos el set de datos:
    from sklearn import preprocessing
    X = preprocessing.StandardScaler().fit(X).transform(X)
    X[0:5]
    
### Entrenar/Probar el set de datos:
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
    print ('Train set:', X_train.shape,  y_train.shape)
    print ('Test set:', X_test.shape,  y_test.shape)
    
### Modelando (Regresi√≥n Log√≠stica con Scikit-learn):
LogisticRegression con el package Scikit-learn. Esta funci√≥n implementa regresi√≥n log√≠stica y puede usar distintos optimizadores num√©ricos para encontrar par√°metros, a saber, ‚Äònewton-cg‚Äô, ‚Äòlbfgs‚Äô, ‚Äòliblinear‚Äô, ‚Äòsag‚Äô, ‚Äòsaga‚Äô solvers. Puedes tambi√©n encontrar m√°s informaci√≥n sobre los pros y contras de estos optimizadores si buscas en internet.

La versi√≥n de Regresi√≥n Log√≠stica en, soporta regularizaci√≥n. Esto es, una t√©cnica que soluciona problemas de sobreajuste en modelos de machine learning. El par√°metro C indica fuerza de regularizaci√≥n inversa la cual debe ser un n√∫mero flotante positivo. Valores m√°s peque√±os indican regularizaci√≥n m√°s fuerte. Now lets fit our model with train set:

    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import confusion_matrix
    LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)    
    LR
    
### predecir usando nuestro set de prueba:
    yhat = LR.predict(X_test)
    yhat

predict_proba devuelve estimaciones para todas las clases. La primer columna es la probabilidad de la clase 1, P(Y=1|X), y la segunda columna es la probabilidad de la clase 0, P(Y=0|X):

    yhat_prob = LR.predict_proba(X_test)
    
### Evaluaci√≥n:
√≠ndice jaccard
Probemos con el √≠ndice jaccard para la evaluaci√≥n de precisi√≥n. Podemos definir como jaccard al tama√±o de la intersecci√≥n dividida por el tama√±o de la uni√≥n de dos set de etiquetas. Si todo el set de etiquetas de muestra predichas coinciden con el set real de etiquetas, entonces la precisi√≥n es 1.0; sino, ser√≠a 0.0.

    from sklearn.metrics import jaccard_similarity_score
    jaccard_similarity_score(y_test, yhat)
    
### Matriz de confusi√≥n: (Otra forma de mirar la precisi√≥n del clasificador es ver la matriz de confusi√≥n.)

    from sklearn.metrics import classification_report, confusion_matrix
    import itertools
    def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    Esta funci√≥n muestra y dibuja la matriz de confusi√≥n.
    La normalizaci√≥n se puede aplicar estableciendo el valor `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Matriz de confusi√≥n normalizada")
    else:
        print('Matriz de confusi√≥n sin normalizaci√≥n')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('Etiqueta Real')
    plt.xlabel('Etiqueta Predicha')
    print(confusion_matrix(y_test, yhat, labels=[1,0]))
    
### Calcular la matriz de confusi√≥n
    cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])
    np.set_printoptions(precision=2)
    
### Dibujar la matriz de confusi√≥n no normalizada
    plt.figure()
    plot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False,  title='Matriz de confusi√≥n')
    
    
<matriz de confusion.png>
    
    print (classification_report(y_test, yhat))
    
    
    ###                precision    recall  f1-score   support

    ###           0       0.73      0.96      0.83        25
    ###           1       0.86      0.40      0.55        15

    ###   micro avg       0.75      0.75      0.75        40
    ###   macro avg       0.79      0.68      0.69        40
    ### weighted avg      0.78      0.75      0.72        40

Partiendo de la cantidad de cada secci√≥n podemos calcular la precisi√≥n y el grado(recall) de cada etiqueta:
- Precision es una medida de certeza basada en una etiqueta predicha. Se define de esta forma: precision = TP / (TP + FP)
- Recall es un grado positivo verdadero. Se define de esta forma: Recall =  TP / (TP + FN)
Por lo tanto, podemos calcular la precisi√≥n y grado de cada clase.
- F1 score: Ahora estamos en condiciones de calcular los puntajes F1 para cada etiqueta basada en la precisi√≥n y grado de cada etiqueta.
El puntaje F1 es el promedio arm√≥nico de la precisi√≥n y grado, donde un grado F1 alcanza su mejor valor en 1 (precisi√≥n y grado perfectos) y peor escenario en 0. Es una buena forma de mostrar que un clasificador tiene un buen valor tanto para la precisi√≥n como para el grado.
Y finalmente, podemos decir que la exactitud promedio para este clasificador es el promedio del score f1 para ambas etiquetas, cuyo valor es is 0.72 en nuestro caso.

### Log Loss:
Ahora, probemos log loss para la evaluaci√≥n. En regresi√≥n log√≠stica, la salida puede ser que la probabilidad de cliente churn sea s√≠ (o su equivalente 1). Esta probabilidad es un valor entre 0 y 1. Log loss( p√©rdida logar√≠tmica) mida el rendimiento de un clasificador donde la salida predicha es una probabilidad de valor entre 0 y 1.

    from sklearn.metrics import log_loss
    log_loss(y_test, yhat_prob)

[Subir](#top)
