<a name='top'></a>
[Principal](../README.md)<br/>

[Documentaci√≥n]<https://www.interactivechaos.com/python/function/traintestsplit>


*[scikit-learn -> regresi√≥n lineal simple](#linealsimple)*<br/>
*[scikit-learn -> regresi√≥n lineal multiple](#multiple)*<br/>
*[scikit-learn -> regresi√≥n polin√≥mica](#polinomica)*<br/>
*[scikit-learn -> regresi√≥n no lineal](#noLineal)*<br/>

<a name='linealsimple'></a>
# Librer√≠a scikit-learn para implementar regresi√≥n lineal simple:
Descargaremos un set de datos relacionado al consumo de combustible y a la emisi√≥n del di√≥xido de Carbono en autos.  Luego, separaremos nuestros datos en un set de entrenamiento y en otro set de prueba, crearemos un modelo utilizando un set de entrenamiento, se evaluar√° utilizando el set de prueba para finalmente usar el modelo para predecir valores desconocidos
### Importando paquetes Necesarios:

    import matplotlib.pyplot as plt
    from sklearn import linear_model
    from sklearn.metrics import r2_score
    import pandas as pd
    import pylab as pl
    import numpy as np
    import wget as wget

### Descargando los Datos:
Para descargar los datos, usaremos !wget desde IBM Object Storage.
    
    url = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv'
    wget.download(url, 'FuelConsumption.csv')

¬øSab√≠as? Cuando se trata de Machine Learning, seguro trabajar√°s con grandes datasets (juego de datos). Entonces, ¬ød√≥nde podr√°s guardar esos datos? IBM ofrece una oportunidad √∫nica para las empresas, con 10 Tb de IBM Cloud Object Storage: Sign up now for free

##### Understanding the Data
FuelConsumption.csv:
Hemos descargado el dataset de consumo de combustible, FuelConsumption.csv, el cual contiene ratings espec√≠ficos al consumo de combustible y emisiones de di√≥xido de carbono para aquellos veh√≠culos ligeros en la venta minorista dentro de Canad√°. Dataset source

    MODELYEAR e.g. 2014
    MAKE e.g. Acura
    MODEL e.g. ILX
    VEHICLE CLASS e.g. SUV
    ENGINE SIZE e.g. 4.7
    CYLINDERS e.g 6
    TRANSMISSION e.g. A6
    FUEL CONSUMPTION in CITY(L/100 km) e.g. 9.9
    FUEL CONSUMPTION in HWY (L/100 km) e.g. 8.9
    FUEL CONSUMPTION COMB (L/100 km) e.g. 9.2
    CO2 EMISSIONS (g/km) e.g. 182 --> low --> 0
   
### Leyendo los datos:
    df = pd.read_csv("FuelConsumption.csv")
    # un vistazo dentro del set de datos
    print(df.head())
    
### Exploraci√≥n de Datos:
    # Tengamos primero una exploraci√≥n descriptiva de nuestros datos.
    # Sumarizar los datos
    print(df.describe())
    
### Seleccionemos algunas caracter√≠sticas para explorar m√°s en detalle:
    cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]
    print(cdf.head(9))
    
### podemos dibujar cada una de estas caracter√≠sticas:
    print(cdf.hist())
    print(plt.show())
   
![React](../Images/RegrsionLS.png)
    
### Ahora, comparemos estas caracter√≠sticas anteriores con la emisi√≥n de carbono, para ver cu√°n lineal es la regresi√≥n:
    plt.scatter(cdf.FUELCONSUMPTION_COMB, cdf.CO2EMISSIONS,  color='blue')
    plt.xlabel("FUELCONSUMPTION_COMB")
    plt.ylabel("Emission")
    print(plt.show())
    
    plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color='blue')
    plt.xlabel("Engine size")
    print(plt.ylabel("Emission"))
    plt.show()
    
    
![React](../Images/ComparacionLS.png)
    
## Creando el set de datos de entrenamiento y de el prueba:
Train/Test Split divide el dataseet en uno de entrenamiento y otro de pruebas, siendo excluyentes. Despu√©s de ello, entrenas con el set de entrenamiento y pruebas con el de prueba. 
Esto brinda una evaluaci√≥n m√°s exacta porque el set de entrenamiento no es parte de un set de datos que se usaron para entrenar datos. Refleja un escenario m√°s real basado en problemas m√°s actuales.
Esto significa que sabemos la salida de cada punto de datos del set, siendo un escenario ideal Y como estos datos no se usaron para entrenar el modelo, el modelo no sabe la salida de estos puntos de datos. Asi que, b√°sicamente, es una prueba real fuera de muestra.

    msk = np.random.rand(len(df)) < 0.8
    train = cdf[msk]
    test = cdf[~msk]
    
![React](../Images/set_datos_entrenamiento_y_prueba.png)
    
## Modelo de Regresi√≥n Simple:
La Regresi√≥n Lineal cuadra con un modelo lineal de coeficientes B = (B1, ..., Bn) para minimizar la 'suma residual de cuadrados' entre la x independiente del dataset y la dependiente y por la aproximaci√≥n lineal.

### Entrenar distribuci√≥n de los datos:
    plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    plt.show()
    
![React](../Images/set_datos_entrenamiento_y_prueba.png)

## Modeling
Usando el paquete sklearn para modelar datos:
    
    regr = linear_model.LinearRegression()
    train_x = np.asanyarray(train[['ENGINESIZE']])
    train_y = np.asanyarray(train[['CO2EMISSIONS']])
    regr.fit (train_x, train_y)
    # The coefficients
    print ('Coefficients: ', regr.coef_)
    print ('Intercept: ',regr.intercept_)
    

son los par√°metros de la recta de ajuste. 
Dado que es una regresi√≥n lineal simple, con 2 par√°metros solamente, y sabiendo que los par√°metros son la intersecci√≥n y pendiente de la linea, sklearn puede estimarlas directamente a partir de los datos. 
Tener en cuenta que todos los datos deben estar disponibles para poder calcular los par√°metros.

## Trazar las salidas
### podemos marcar la recta de ajuste sobre los datos:
    plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
    plt.plot(train_x, regr.coef_[0][0]*train_x + regr.intercept_[0], '-r')
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    
## Evaluaci√≥n:
comparamos los valores actuales y predichos para calcular la exactitud del modelo de regresi√≥n. Las m√©tricas de la evaluaci√≥n proveen un role principal en el #### desarrollo de un modelo, ya que provee conocimiento profundo en areas que necesitan mejoras.

    test_x = np.asanyarray(test[['ENGINESIZE']])
    test_y = np.asanyarray(test[['CO2EMISSIONS']])
    test_y_ = regr.predict(test_x)

    print("Error medio absoluto: %.2f" % np.mean(np.absolute(test_y_ - test_y)))
    print("Suma residual de los cuadrados (MSE): %.2f" % np.mean((test_y_ - test_y) ** 2))
    print("R2-score: %.2f" % r2_score(test_y_ , test_y) )


<a name='multiple'></a>
*[scikit-learn -> regresi√≥n lineal simple](#linealsimple)*<br/>
*[scikit-learn -> regresi√≥n lineal multiple](#multiple)*<br/>
*[scikit-learn -> regresi√≥n polin√≥mica](#polinomica)*<br/>
*[scikit-learn -> regresi√≥n no lineal](#noLineal)*<br/>

# Librer√≠a scikit-learn para implementar regresi√≥n lineal multiple:
### Importando paquetes Necesarios:

    import matplotlib.pyplot as plt
    from sklearn import linear_model
    import pandas as pd
    import numpy as np
    import wget as wget

### Descargando los Datos:
Para descargar los datos, usaremos !wget desde IBM Object Storage.
    
    url = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv'
    wget.download(url, 'FuelConsumption.csv')

¬øSab√≠as? Cuando se trata de Machine Learning, seguro trabajar√°s con grandes datasets (juego de datos). Entonces, ¬ød√≥nde podr√°s guardar esos datos? IBM ofrece una oportunidad √∫nica para las empresas, con 10 Tb de IBM Cloud Object Storage: Sign up now for free

##### Understanding the Data
FuelConsumption.csv:
Hemos descargado el dataset de consumo de combustible, FuelConsumption.csv, el cual contiene ratings espec√≠ficos al consumo de combustible y emisiones de di√≥xido de carbono para aquellos veh√≠culos ligeros en la venta minorista dentro de Canad√°. Dataset source

    MODELYEAR e.g. 2014
    MAKE e.g. Acura
    MODEL e.g. ILX
    VEHICLE CLASS e.g. SUV
    ENGINE SIZE e.g. 4.7
    CYLINDERS e.g 6
    TRANSMISSION e.g. A6
    FUEL CONSUMPTION in CITY(L/100 km) e.g. 9.9
    FUEL CONSUMPTION in HWY (L/100 km) e.g. 8.9
    FUEL CONSUMPTION COMB (L/100 km) e.g. 9.2
    CO2 EMISSIONS (g/km) e.g. 182 --> low --> 0
   
### Leyendo los datos:
    df = pd.read_csv("FuelConsumption.csv")
    # un vistazo dentro del set de datos
    print(df.head())
    
### Seleccionemos algunas caracter√≠sticas:
    cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_CITY','FUELCONSUMPTION_HWY','FUELCONSUMPTION_COMB','CO2EMISSIONS']]
    print(cdf.head(9))
    
### Tracemos los valores de las emisiones con respecto al tama√±o del motor:
    plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color='blue')
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    print(plt.show())
    
![React](../Images/regresion_lineal_multiple.png)

## Creating train and test dataset
La divisi√≥n tren/prueba implica dividir el conjunto de datos en conjuntos de formaci√≥n y de pruebas respectivamente, que son mutuamente excluyentes. Despu√©s de lo cual, usted entrena con el equipo de entrenamiento y prueba con el equipo de prueba. Esto proporcionar√° una evaluaci√≥n m√°s precisa de la precisi√≥n fuera de la muestra, ya que el conjunto de datos de la prueba no forma parte del conjunto de datos que se ha utilizado para entrenar los datos. Es m√°s realista para los problemas del mundo real.
    
    msk = np.random.rand(len(df)) < 0.8
    train = cdf[msk]
    test = cdf[~msk]
    
#### Train data distribution
    plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    print(plt.show())
    
![React](../Images/distribucionlinealmultiple.png)
    
## Multiple Regression Model: 
Cuando hay m√°s de una variable independiente presente, el proceso se denomina regresi√≥n lineal m√∫ltiple. Lo bueno aqu√≠ es que la regresi√≥n lineal m√∫ltiple es la extensi√≥n del modelo de regresi√≥n lineal simple.
    
    regr = linear_model.LinearRegression()
    x = np.asanyarray(train[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])
    y = np.asanyarray(train[['CO2EMISSIONS']])
    regr.fit (x, y)
    # The coefficients
    print ('Coefficients: ', regr.coef_)

## Ordinary Least Squares (OLS)
OLS es un m√©todo para estimar los par√°metros desconocidos en un modelo de regresi√≥n lineal. OLS elige los par√°metros de una funci√≥n lineal de un conjunto de variables explicativas minimizando la suma de los cuadrados de las diferencias entre la variable objetivo dependiente y las previstas por la funci√≥n lineal. En otras palabras, intenta minimizar la suma de errores cuadrados (SSE) o el error cuadrado medio (MSE) entre la variable objetivo (y) y nuestro resultado previsto ( ‚Ñéùëéùë°‚Ñéùëéùë°ùë¶ ) en todas las muestras del conjunto de datos.

OLS puede encontrar los mejores par√°metros usando los siguientes m√©todos: 
- Resoluci√≥n anal√≠tica de los par√°metros del modelo mediante ecuaciones de forma cerrada 
- Utilizando un algoritmo de optimizaci√≥n (Descenso de Gradiente, Descenso de Gradiente Estoc√°stico, M√©todo de Newton, etc.)

#### Prediction:
    y_hat= regr.predict(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])
    x = np.asanyarray(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])
    y = np.asanyarray(test[['CO2EMISSIONS']])
    print("Residual sum of squares: %.2f"
      % np.mean((y_hat - y) ** 2))

    # Explained variance score: 1 is perfect prediction
    print('Variance score: %.2f' % regr.score(x, y))


<a name='polinomica'></a>
*[scikit-learn -> regresi√≥n lineal simple](#linealsimple)*<br/>
*[scikit-learn -> regresi√≥n lineal multiple](#multiple)*<br/>
*[scikit-learn -> regresi√≥n polin√≥mica](#polinomica)*<br/>
*[scikit-learn -> regresi√≥n no lineal](#noLineal)*<br/>

## Regresi√≥n polin√≥mica:
Implementar una Regresi√≥n Polin√≥mica. Descargaremos un set de datos relacionado al consumo de combustible y a la emisi√≥n del di√≥xido de Carbono en autos. Luego, separaremos nuestros datos en un set de entrenamiento y en otro set de prueba, crearemos un modelo utilizando un set de entrenamiento, se evaluar√° utilizando el set de prueba para finalmente usar el modelo para predecir valores desconocidos

### Importando los paquetes necesarios:
   
    import matplotlib.pyplot as plt
    from sklearn import linear_model
    from sklearn.metrics import r2_score
    import pandas as pd
    import pylab as pl
    import numpy as np
    import wget as wget

### Descargando los Datos:
Para descargar los datos, usaremos !wget desde IBM Object Storage.
    
    url = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv'
    wget.download(url, 'FuelConsumption.csv')

¬øSab√≠as? Cuando se trata de Machine Learning, seguro trabajar√°s con grandes datasets (juego de datos). Entonces, ¬ød√≥nde podr√°s guardar esos datos? IBM ofrece una oportunidad √∫nica para las empresas, con 10 Tb de IBM Cloud Object Storage: Sign up now for free

##### Understanding the Data
FuelConsumption.csv:
Hemos descargado el dataset de consumo de combustible, FuelConsumption.csv, el cual contiene ratings espec√≠ficos al consumo de combustible y emisiones de di√≥xido de carbono para aquellos veh√≠culos ligeros en la venta minorista dentro de Canad√°. Dataset source

    MODELYEAR e.g. 2014
    MAKE e.g. Acura
    MODEL e.g. ILX
    VEHICLE CLASS e.g. SUV
    ENGINE SIZE e.g. 4.7
    CYLINDERS e.g 6
    TRANSMISSION e.g. A6
    FUEL CONSUMPTION in CITY(L/100 km) e.g. 9.9
    FUEL CONSUMPTION in HWY (L/100 km) e.g. 8.9
    FUEL CONSUMPTION COMB (L/100 km) e.g. 9.2
    CO2 EMISSIONS (g/km) e.g. 182 --> low --> 0
   
### Leyendo los datos:
    df = pd.read_csv("FuelConsumption.csv")
    # un vistazo dentro del set de datos
    print(df.head())

### Seleccionemos algunas caracater√≠sticas para usar en la regresi√≥n:
    cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]
    print(cdf.head(9))
    
### Grafiquemos los valores de emisi√≥n respecto al tama√±o del motor:
    plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color='blue')
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    print(plt.show())
    
![React](../Images/polinomica.png)
    
### Crear conjunto de datos de entrenamiento y pruebas:
Hay que dividir el conjunto en dos, el de entrenamiento y el de pruebas, los cuales son mutuamente excluyentes. Despues de hacerlo, deber√° entrenar con el conjunto de entrenamiento y hacer pruebas con el conjunto de pruebas.
    
    msk = np.random.rand(len(df)) < 0.8
    train = cdf[msk]
    test = cdf[~msk]

## Regresi√≥n Polin√≥mica:
En ocasiones la tendencia de los datos no es lineal si no que tiene una apariencia curva. Para estos caso podemos usar los m√©todos de Regresi√≥n Polin√≥mica. De hecho, existen diversos tipos de regresi√≥n que pueden ser usados para ajustarse de acuerdo a la apariencia de los datos, como puede ser la regresi√≥n cuadratica, c√∫bica, etc. Puede haber tantos tipos de regresiones como grados en un polinomio.

La funci√≥n PloynomialFeatures() de la librer√≠a Scikit-learn maneja un nuevo conjunto de caracter√≠sticas del conjunto original.

    train_x = np.asanyarray(train[['ENGINESIZE']])
    train_y = np.asanyarray(train[['CO2EMISSIONS']])

    test_x = np.asanyarray(test[['ENGINESIZE']])
    test_y = np.asanyarray(test[['CO2EMISSIONS']])

    poly = PolynomialFeatures(degree=2)
    #fit_transform toma los valores de x e imprime una lista de los datos que van desde la magnitud 0 a la 2 (ya que hemos seleccionado que nuestro polin√≥mio   sea de segundo grado).
    train_x_poly = poly.fit_transform(train_x)
    print(train_x_poly)

Ahora podemos manejar el problema como si se tratara de una 'regresi√≥n lineal'. Por lo tanto, esta regresi√≥n polinomica se considera como un caso especial de regresi√≥n lineal m√∫ltiple. Puede utilizar la misma mec√°nica para resolver dicho problema.

### Usemos la funci√≥n LinearRegression() para resolver:
    clf = linear_model.LinearRegression()
    train_y_ = clf.fit(train_x_poly, train_y)
    # los coeficientes 
    print ('Coefficients: ', clf.coef_)
    print ('Intercept: ',clf.intercept_)

### Grafiquemoslo:
    plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
    XX = np.arange(0.0, 10.0, 0.1)
    yy = clf.intercept_[0]+ clf.coef_[0][1]*XX+ clf.coef_[0][2]*np.power(XX, 2)
    plt.plot(XX, yy, '-r' )
    plt.xlabel("Engine size")
    plt.ylabel("Emission")
    print(plt.show())
    
![React](../Images/polinomicaGRF.png)

### Evaluaci√≥n:
    test_x_poly = poly.fit_transform(test_x)
    test_y_ = clf.predict(test_x_poly)

    print("Mean absolute error: %.2f" % np.mean(np.absolute(test_y_ - test_y)))
    print("Residual sum of squares (MSE): %.2f" % np.mean((test_y_ - test_y) ** 2))
    print("R2-score: %.2f" % r2_score(test_y_ , test_y) )
    
### uso de la regresi√≥n polinomica de tercer grado(c√∫bica) para mayor precisi√≥n:
    poly3 = PolynomialFeatures(degree=3)
    train_x_poly3 = poly3.fit_transform(train_x)
    clf3 = linear_model.LinearRegression()
    train_y3_ = clf3.fit(train_x_poly3, train_y)

    # The coefficients
    print('Coefficients: ', clf3.coef_)
    print('Intercept: ', clf3.intercept_)
    plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS, color='blue')
    XX = np.arange(0.0, 10.0, 0.1)
    yy = clf3.intercept_[0] + clf3.coef_[0][1] * XX + clf3.coef_[0][2] * np.power(XX, 2) + clf3.coef_[0][3] * np.power(XX, 3)
    plt.plot(XX, yy, '-r')
    plt.xlabel("Engine size")   
    plt.ylabel("Emission")
    print(plt.show())
    
![React](../Images/polinomica3.png)

    test_x_poly3 = poly3.fit_transform(test_x)
    test_y3_ = clf3.predict(test_x_poly3)
    print("Mean absolute error: %.2f" % np.mean(np.absolute(test_y3_ - test_y)))
    print("Residual sum of squares (MSE): %.2f" % np.mean((test_y3_ - test_y) ** 2))
    print("R2-score: %.2f" % r2_score(test_y3_, test_y))

<a name='noLineal'></a>
*[scikit-learn -> regresi√≥n lineal simple](#linealsimple)*<br/>
*[scikit-learn -> regresi√≥n lineal multiple](#multiple)*<br/>
*[scikit-learn -> regresi√≥n polin√≥mica](#polinomica)*<br/>
*[scikit-learn -> regresi√≥n no lineal](#noLineal)*<br/>
## Regresion no lienal:
### Importando las librer√≠as requeridas:
    import numpy as np
    import matplotlib.pyplot as plt
    %matplotlib inline

Las regresiones no-lineales son una relaci√≥n entre variables independientes  ùë•  y una variable dependiente  ùë¶  que resulta en una funci√≥n no lineal. B√°sicamente, cada relaci√≥n que no es lineal puede transformarse en una no lineal, y generalmente se representa con el polinomio de grados  ùëò  (potencia m√°xima de  ùë• ).
    ùë¶=ùëéùë•3+ùëèùë•2+ùëêùë•+ùëë 
 
Las funciones no lineales pueden tener elementos como exponentes, logaritmos, fracciones y otros. Por ejemplo:
    ùë¶=log(ùë•)
 
O m√°s complicados, como :
    ùë¶=log(ùëéùë•3+ùëèùë•2+ùëêùë•+ùëë)

### gr√°fico de la funci√≥n c√∫bica:
    x = np.arange(-5.0, 5.0, 0.1)

    ##Puede ajustar la pendiente y la intersecci√≥n para verificar los cambios del gr√°fico
    y = 1*(x**3) + 1*(x**2) + 1*x + 3
    y_noise = 20 * np.random.normal(size=x.size)
    ydata = y + y_noise
    plt.plot(x, ydata,  'bo')
    plt.plot(x,y, 'r') 
    plt.ylabel('Variable dependiente')
    plt.xlabel('Variable indepdendiente')
    print(plt.show())
    
![React](../Images/nolinealcubica.png)
    
Como se puede ver, esta funci√≥n tiene  ùë•3  y  ùë•2  como variables independientes. Tambi√©n, el gr√°fico de esta funci√≥n no es una linea directa, por lo que es una funci√≥n no lineal.

## Algunas otras funciones no lineales son:
### Cuadr√°tica ùëå=ùëã2
    x = np.arange(-5.0, 5.0, 0.1)

    ##Se puede ajustar la pendiente y la intersecci√≥n para verificar los cambios en el gr√°fico
    y = np.power(x,2)
    y_noise = 2 * np.random.normal(size=x.size)
    ydata = y + y_noise
    plt.plot(x, ydata,  'bo')
    plt.plot(x,y, 'r') 
    plt.ylabel('Variable dependiente')
    plt.xlabel('Variable indepdiendente')
    plt.show()

![React](../Images/nolinealcuadratica.png)

### Exponencial
Una funci√≥n exponencial con base c se define por
ùëå=ùëé+ùëèùëêùëã

donde b ‚â†0, c > 0 , c ‚â†1, y x es cualquier n√∫mero real. La base, c, es constante y el exponente, x, es una variable.
    
    X = np.arange(-5.0, 5.0, 0.1)

    ##Se puede ajustar la pendiente y la intersecci√≥n para verificar los cambios en el gr√°fico
    Y= np.exp(X)
    plt.plot(X,Y) 
    plt.ylabel('Variable Dependiente')
    plt.xlabel('Variable Independiente')
    plt.show()
    
![React](../Images/nolinealexponencial.png)
    
### Logar√≠tmico
La respuesta  ùë¶  es el resultado de aplicar el mapa logar√≠tmico desde el valor de entrada de  ùë•  a la variable de salida  ùë¶ . 
Es una de las formas m√°s simples de 
log(): i.e. ùë¶=log(ùë•)
 
considerar que en vez de  ùë• , podemos usar  ùëã , el cual puede ser una representaci√≥n polinomial de las  ùë• 's. En su forma general, se escribir√≠a como 
ùë¶=log(ùëã)

    X = np.arange(-5.0, 5.0, 0.1)
    
    ##Se puede ajustar la pendiente y la intersecci√≥n para verificar los cambios en el gr√°fico
    Y = np.log(X)
    plt.plot(X,Y) 
    plt.ylabel('Variable Dependiente')
    plt.xlabel('Variable Independiente')
    plt.show()
    
![React](../Images/nolineallogaritmica.png)
    
### Sigmoidal/Log√≠stica ùëå=ùëé+ùëè1+ùëê(ùëã‚àíùëë)
    X = np.arange(-5.0, 5.0, 0.1)

    Y = 1-4/(1+np.power(3, X-2))

    plt.plot(X,Y) 
    plt.ylabel('Variable Dependiente')
    plt.xlabel('Variable Independiente')
    plt.show()
        
![React](../Images/nolinealsigmoidal_logistica.png)
    
### Ejemplo Regresi√≥n No-Lineal:
Por ejemplo, intentaremos fijar un modelo no lineal a los puntos correspondientes al GDP de China entre los a√±os 1960 y 2014. Descargaremos un set de datos con dos columnas, la primera, un a√±o entre 1960 y 2014, la segunda, el ingreso anual de China en d√≥lares estadounidenses para ese a√±o.
    
    import numpy as np
    import pandas as pd
    import wget as wget
    import matplotlib.pyplot as plt
    from sklearn.metrics import r2_score
    from scipy.optimize import curve_fit

### downloading dataset
    url = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/china_gdp.csv'
    wget.download(url, 'china_gdp.csv')
    
### leemos los datos obtenidos y mostramos los 10 primeros:
    df = pd.read_csv("china_gdp.csv")
    df.head(10)
    
### Marcando el set de datos:
    plt.figure(figsize=(8,5))
    x_data, y_data = (df["Year"].values, df["Value"].values)
    plt.plot(x_data, y_data, 'ro')
    plt.ylabel('GDP')
    plt.xlabel('Year')
    plt.show()
    
![React](../Images/nolineal.png)
    
### Eligiendo un modelo:
    X = np.arange(-5.0, 5.0, 0.1)
    Y = 1.0 / (1.0 + np.exp(-X))

    plt.plot(X,Y) 
    plt.ylabel('Variable Dependiente')
    plt.xlabel('Variable Independiente')
    plt.show()
    
![React](../Images/selectmodelnolineal.png)
    
### Construyendo el Modelo:
    def sigmoid(x, Beta_1, Beta_2):
        y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))
        return y

### sigmoide posible:
    beta_1 = 0.10
    beta_2 = 1990.0

    #funci√≥n log√≠stica
    Y_pred = sigmoid(x_data, beta_1 , beta_2)

    #predicci√≥n de puntos
    plt.plot(x_data, Y_pred*15000000000000.)
    plt.plot(x_data, y_data, 'ro')
    
### busqueda de mejores par√°metros y normalizar x e y:
    # Normalicemos nuestros datos
    xdata =x_data/max(x_data)
    ydata =y_data/max(y_data)

### ¬øC√≥mo podemos encontrar los mejores par√°metros para nuestra linea?
podemos utilizar curve_fit la cual utiliza cuadrados m√≠nimos no lineales para cuadrar con la funci√≥n sigmoide
popt son nuestros par√°metros optimizados.

    from scipy.optimize import curve_fit
    popt, pcov = curve_fit(sigmoid, xdata, ydata)
    #imprimir los par√°metros finales
    print(" beta_1 = %f, beta_2 = %f" % (popt[0], popt[1]))
    
### Dibujamos nuestro modelo de regresi√≥n:
    x = np.linspace(1960, 2015, 55)
    x = x/max(x)
    plt.figure(figsize=(8,5))
    y = sigmoid(x, *popt)
    plt.plot(xdata, ydata, 'ro', label='data')
    plt.plot(x,y, linewidth=3.0, label='fit')
    plt.legend(loc='best')
    plt.ylabel('GDP')
    plt.xlabel('Year')
    plt.show()
    
![React](../Images/nolinealgdpyear.png)
![React](../Images/ajustandomodelonolienalgdpyear.png)

###  calcular la exactitud del modelo:
    msk = np.random.rand(len(df)) < 0.8
    train_x = xdata[msk]
    test_x = xdata[~msk]
    train_y = ydata[msk]
    test_y = ydata[~msk]

### construye el modelo utilizando el set de entrenamiento
    popt, pcov = curve_fit(sigmoid, train_x, train_y)

### predecir utilizando el set de prueba
    y_hat = sigmoid(test_x, *popt)

### evaluation:
    print("Promedio de error absoluto: %.2f" % np.mean(np.absolute(y_hat - test_y)))
    print("Suma residual de cuadrados (MSE): %.2f" % np.mean((y_hat - test_y) ** 2))
    print("R2-score: %.2f" % r2_score(y_hat , test_y) )

[Subir](#top)
